{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import ollama\n",
    "import textwrap\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        if soup.body:\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            self.text = \"\"\n",
    "        self.links = [link.get('href') for link in soup.find_all('a') if link.get('href')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebsiteChatbot:\n",
    "    def __init__(self, website):\n",
    "        self.website = website\n",
    "\n",
    "    def ask(self, question):\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI assistant with knowledge about the following website: {self.website.url}.\n",
    "        Website Title: {self.website.title}\n",
    "        Extracted Content:\n",
    "        {self.website.text[:3000]}  # Limiting to 3000 chars for context\n",
    "        \n",
    "        User Question: {question}\n",
    "        Answer in well-formatted Markdown style:\n",
    "        \"\"\"\n",
    "        response = ollama.chat(model=\"llama3.2\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        formatted_response = textwrap.fill(response[\"message\"][\"content\"], width=180)\n",
    "        return formatted_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_website(url, question):\n",
    "    if not url.strip():\n",
    "        return \"Please enter a valid website URL.\"\n",
    "    if not question.strip():\n",
    "        return \"Please enter a question.\"\n",
    "    \n",
    "    site = Website(url)\n",
    "    chatbot = WebsiteChatbot(site)\n",
    "    return chatbot.ask(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=chat_with_website,\n",
    "    inputs=[gr.Textbox(label=\"Website URL\"), gr.Textbox(label=\"Your Question\")],\n",
    "    outputs=gr.Markdown(label=\"AI Response\"),\n",
    "    title=\"Website Chatbot with LLaMA 3.2\",\n",
    "    description=\"Enter a website URL and ask a question about its content.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is being done with llama3.2 which is agood model, its open source and so you can run it locally at no cost. However, we would get much better results if we used gpt-4o-mini because it is more powerful but then you need to pay for the API - nevertheless this is a useful tool to save you a lot of time. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
